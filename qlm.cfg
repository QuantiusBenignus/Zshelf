#-----Local qlm Configuration, adjust as/when needed:
#local inference engine, assuming llama.cpp (`llam` is a symbolic link to `llama-cli`), options preset for it: 
local inferengine='llam'
#Location of local language models:
LLMDIR="$HOME/LLM/Models"
#Default model name (a valid key in llmodels, defined in qlm.cfg) when no model name provided:
local defaultlm='Gemma3_12B'
#Local LLM prompt collection file $TPROMPTF, a temp file used for context ingestion and prompting:
TPROMPTF='/dev/shm/promf'
#NOTE: If $1 and TPROMPTF don't exist, the code will look for any selected text and place it in the context.
#File to store the LLM name called last (used by reqlm, persists across sessions)
lastllmf='/dev/shm/lastqllm'

#LLM inference functions gem, gem2, qwen, qwec, mist and deeq are defined in .zfunc, for autoloading:
alias reqwen='() {[[ -f /dev/shm/reqwen ]] && qwen "$(cat /dev/shm/reqwen)\n$1" || print "No previous qwen calls found!" ; }'
alias reqwec='() {[[ -f /dev/shm/reqwec ]] && qwec "$(cat /dev/shm/reqwec)\n$1" || print "No previous qwec calls found!" ; }'
alias reqwec='() {[[ -f /dev/shm/reqwec ]] && qwec "$(cat /dev/shm/reqwec)\n$1" || print "No previous qwec calls found!" ; }'
alias remist='() {[[ -f /dev/shm/remist ]] && mist "$(cat /dev/shm/remist)\n$1" || print "No previous mist calls found!" ; }'
alias redeeq='() {[[ -f /dev/shm/redeeq ]] && deeq "$(cat /dev/shm/redeeq)\n$1" || print "No previous deeq calls found!" ; }'
alias regem='() {[[ -f /dev/shm/regem ]] && gem "$(cat /dev/shm/regem)\n$1" || print "No previous gem calls found!" ; }'
alias regem2='() {[[ -f /dev/shm/regem2 ]] && gem2 "$(cat /dev/shm/regem2)\n$1" || print "No previous gem2 calls found!" ; }'

#Here we define the large language models and their parameters. When adding a model, update all arrays correspondingly.
#Avaliable model files (adjust to your situation) that you want to choose from to use with the qlm function and its completion function:
#Declared as global assoc. array to speed up completion ["llmname"]="llmfilename":
typeset -gA llmodels=(
["DeepseekQwen_14B"]="DeepSeek-R1-Distill-Qwen-14B-Q5_K_L.gguf"
["Gemma3_1B"]="gemma-3-1b-it-Q6_K_L.gguf"
["Gemma3_4B"]="gemma-3-4b-it-Q6_K_L.gguf"
["Gemma3_12B"]="gemma-3-12b-it-Q6_K_L.gguf"
["Gemma3_27B"]="gemma-3-27b-it-Q5_K_L.gguf"
["Gemma3_27IB"]="gemma-3-27b-it-IQ3_XXS.gguf"
["Gemma2_9B"]="gemma-2-9b-it-Q6_K_L.gguf"
["Gemma2_2B"]="gemma-2-2b-it-abliterated-Q6_K_L.gguf"
["Gemma2_27B"]="gemma-2-27b-it-IQ4_XS.gguf"
["MistralSmall3_24B"]="Mistral-Small-24B-Instruct-2501-IQ4_XS.gguf"
["QwenCoder_14B"]="Qwen2.5-Coder-14B-Instruct-Q5_K_L.gguf"
["QwenCoder_32B"]="Qwen2.5-Coder-32B-Instruct-Q5_K_L.gguf"
["Qwen_14B"]="Qwen2.5-14B-Instruct-Q5_K_L.gguf"
["QQwQ_32B"]="Qwen_QwQ-32B-Q5_K_L.gguf"
["Phi4_14B"]="phi-4-Q5_K_L.gguf"
["Phi4_mini"]="Phi-4-mini-instruct-Q6_K_L.gguf"
["Granite3.2_2B"]="granite-3.2-2b-instruct-Q8_0.gguf"
["Llama3.1_8B"]="Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf"
["Llama3.2_3B"]="Llama-3.2-3B-Instruct-Q6_K_L.gguf"
)

#Desired context length (powers of 2), to be adjusted depending on use, model size, VRAM size and gpulayers.            
typeset -A ctxsize=(
["DeepseekQwen_14B"]=4096
["Gemma3_1B"]=32768
["Gemma3_4B"]=16384
["Gemma3_12B"]=6144
["Gemma3_27B"]=6144
["Gemma3_27IB"]=8192
["Gemma2_9B"]=8192
["Gemma2_2B"]=8192
["Gemma2_27B"]=4096
["MistralSmall3_24B"]=8192
["QwenCoder_14B"]=4096
["QwenCoder_32B"]=8192
["Qwen_14B"]=4096
["QQwQ_32B"]=8192
["Phi4_14B"]=4096
["Phi4_mini"]=32768
["Granite3.2_2B"]=4096
["Llama3.1_8B"]=8192
["Llama3.2_3B"]=8192
)
#Max context length of each model:
#DeepseekQwen_14B 131072
#Gemma3_1B 32768, 26 blocks
#Gemma3_4B 131072, 48 blocks
#Gemma3_12B 131072, 48 blocks
#Gemma3_27B 131072, 62 blocks
#Gemma3_27IB 131072, 62 blocks (fits completely in 12GB VRAM)
#Gemma2_9B 8192
#Gemma2_2B 8192
#Gemma2_27B 8192
#MistralSmall3_24B 32768
#QwenCoder_14B 131072
#QwenCoder_32B 131072
#Qwen_14B 131072
#QQwQ_32B 131072
#Phi4_14B 16384
#Phi4_mini 131072
#Granite3.2_2B 131072 
#Llama3.1_8B 131072
#Llama3.2_3B 131072

#Layers to offload to the GPU, to be adjusted depending on model size, VRAM size and desired ctxsize:
typeset -A gpulayers=(
["DeepseekQwen_14B"]=99
["Gemma3_1B"]=99
["Gemma3_4B"]=99
["Gemma3_12B"]=99
["Gemma3_27B"]=34
["Gemma3_27IB"]=99
["Gemma2_9B"]=99
["Gemma2_2B"]=99
["Gemma2_27B"]=30
["MistralSmall3_24B"]=29
["QwenCoder_14B"]=99
["QwenCoder_32B"]=28
["Qwen_14B"]=99
["QQwQ_32B"]=28
["Phi4_14B"]=99
["Phi4_mini"]=99
["Granite3.2_2B"]=99
["Llama3.1_8B"]=99
["Llama3.2_3B"]=99
)

#Preset temperature parameters for inference, depend on the model and the task:
typeset -A temps=(
["DeepseekQwen_14B"]=0.61
["Gemma3_1B"]=0.16
["Gemma3_4B"]=0.16
["Gemma3_12B"]=0.16
["Gemma3_27B"]=0.16
["Gemma3_27IB"]=0.1
["Gemma2_9B"]=0.16
["Gemma2_2B"]=0.16
["Gemma2_27B"]=0.16
["MistralSmall3_24B"]=0.05
["QwenCoder_14B"]=0.5
["QwenCoder_32B"]=0.5
["Qwen_14B"]=0.61
["QQwQ_32B"]=0.5
["Phi4_14B"]=0.61
["Phi4_mini"]=0.61
["Granite3.2_2B"]=0.61
["Llama3.1_8B"]=0.61
["Llama3.2_3B"]=0.61
)

#Model-dependent prompts with placeholder (UIUIUI) for context(user prompt) ingestion:
typeset -A llmprompts=(
["DeepseekQwen_14B"]="<｜begin▁of▁sentence｜>You are DeepSeekR1-Qwen14B. You are a helpful assistant.<｜User｜>UIUIUI<｜Assistant｜>"
["Gemma3_1B"]="<bos><start_of_turn>user\nYou are Gemma3. You are helpful assistant who answers briefly and to the point.\nUIUIUI<end_of_turn>\n<start_of_turn>model"
["Gemma3_4B"]="<bos><start_of_turn>user\nYou are Gemma3. You are helpful assistant who answers briefly and to the point.\nUIUIUI<end_of_turn>\n<start_of_turn>model"
["Gemma3_12B"]="<bos><start_of_turn>user\nYou are Gemma3. You are helpful assistant who answers briefly and to the point.\nUIUIUI<end_of_turn>\n<start_of_turn>model"
["Gemma3_27B"]="<bos><start_of_turn>user\nYou are Gemma3. You are helpful assistant who answers briefly and to the point.\nUIUIUI<end_of_turn>\n<start_of_turn>model"
["Gemma3_27IB"]="<bos><start_of_turn>user\nYou are Gemma3. You are helpful assistant who answers briefly and to the point.\nUIUIUI<end_of_turn>\n<start_of_turn>model"
["Gemma2_9B"]="User:\nUIUIUI\nAssistant:"
["Gemma2_2B"]="User:\nUIUIUI\nAssistant:"
["Gemma2_27B"]="User:\nUIUIUI\nAssistant:"
["MistralSmall3_24B"]="[SYSTEM_PROMPT]You are Mistral Small 3, a Large Language Model created by Mistral AI. When not sure about some information, you say that you do not know.[/SYSTEM_PROMPT][INST]UIUIUI[/INST]"
["QwenCoder_14B"]="<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful coding assistant.<|im_end|>\n<|im_start|>user\nUIUIUI<|im_end|>\n<|im_start|>assistant\n"
["QwenCoder_32B"]="<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful coding assistant.<|im_end|>\n<|im_start|>user\nUIUIUI<|im_end|>\n<|im_start|>assistant\n"
["Qwen_14B"]="<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nUIUIUI<|im_end|>\n<|im_start|>assistant\n"
["QQwQ_32B"]="<|im_start|>system\nYou are Qwen, a helpful coding assistant. Think step by step but only keep a minimum draft of each thinking step, with 5 words at most. Return the answer at the end of the response after a separator ####.<|im_end|>\n<|im_start|>user\nUIUIUI<|im_end|>\n<|im_start|>assistant\n"
["Phi4_14B"]="<|im_start|>system<|im_sep|>You are Phi4, a helpful assistant.<|im_end|><|im_start|>user<|im_sep|>UIUIUI<|im_end|><|im_start|>assistant<|im_sep|>"
["Phi4_mini"]="<|system|>You are Phi4-mini, a helpful assistant.<|end|><|user|>UIUIUI<|end|><|assistant|>"
["Granite3.2_2B"]="<|start_of_role|>system<|end_of_role|>You are Granite, developed by IBM. You are a helpful AI assistant. \nRespond to every user query in a detailed way. You can share your thoughts and reasoning before responding. In the thought process, engage in a comprehensive, iterative cycle of analysis, summarization, exploration, reassessment and reflection. In the response, based on explorations and reflections from the thoughts section, systematically present the final solution that you deem correct. The response should summarize the thought process for each user query.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>UIUIUI<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>\n"
["Llama3.1_8B"]="<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\nToday Date: $(date +'%d %B %Y')\nYou are Llama3.1, a helpfull assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>UIUIUI<|eot_id|><|start_header_id|>assistant<|end_header_id|>"
["Llama3.2_3B"]="Today is $(date +'%d %b %Y')\nYou are Llama3.2, a helpfull assistant.\nUIUIUI"
)

#The next arrays are used for dynamic adjustment of `-ngl` for models that DO NOT fit in the GPU VRAM (NVIDIA only for now, specifically for 12GB VRAM):
#VRAM load as a function of offloaded layers (up to $gpulayers[$ModelName]) for the models that do not fit the resident GPU (12288 MB in this example, adjust for your case)
#These data were obtained empirically with the gpulayers.zsh script. To avoid using this dynamic layer feature, comment the code block marked #Dynamic... in qlm, or create a dummy array:
#Model_Name=(1 1 1 1 1 1 1 .. 1 1 1) #where the number of elements is -ge $gpulayers[Model_Name]
Gemma3_27B=(2510 2796 3084 3370 3658 3944 4230 4518 4788 5060 5346 5616 5888 6174 6446 6718 7004 7276 7546 7834 8104 8374 8662 8932 9204 9489 9759 10031 10317 10589 10859 11145 11417 11687)
Gemma2_27B=(2032 2318 2606 2893 3181 3467 3755 4041 4328 4614 4902 5188 5476 5762 6050 6336 6624 6910 7175 7463 7749 8037 8323 8611 8897 9185 9471 9759 10045 10333 10619 10907 11193 11481 11822)
MistralSmall3_24B=(1520 1802 2084 2366 2648 2932 3214 3496 3776 4058 4342 4624 4906 5188 5470 5754 6036 6318 6600 6882 7166 7448 7712 7994 8276 8558 8842 9124 9406 9688 9970 10254 10536 10818 11100 11382 11666)
QwenCoder_32B=(1716 2054 2392 2730 3068 3408 3746 4079 4416 4736 5056 5394 5714 6034 6372 6692 7012 7324 7644 7964 8302 8622 8942 9280 9600 9918 10258 10578 10896 11236 11554)
QQwQ_32B=(1716 2054 2392 2730 3068 3408 3746 4079 4416 4736 5056 5394 5714 6034 6372 6692 7012 7324 7644 7964 8302 8622 8942 9280 9600 9918 10258 10578 10896 11236 11554)
