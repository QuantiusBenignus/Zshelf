#-----Local qlm Configuration, adjust as/when needed:
#local inference engine, assuming llama.cpp (`llam` is a symbolic link to `llama-cli`), options preset for it: 
local inferengine='llam'
#Location of local language models:
LLMDIR="$HOME/LLM/Models"
#Default model name (a valid key in llmodels, defined in qlm.cfg) when no model name provided:
local defaultlm='Qwen_14B'
#Local LLM prompt collection file $TPROMPTF, a temp file used for context ingestion and prompting:
TPROMPTF='/dev/shm/promf'
#NOTE: If $1 and TPROMPTF don't exist, the code will look for any selected text and place it in the context.
#File to store the LLM name called last (used by reqlm, persists across sessions)
lastllmf='/dev/shm/lastqllm'

#LLM inference functions gem, gem2, qwen, qwec, mist and deeq are defined in .zfunc, for autoloading:
alias reqwen='() {[[ -f /dev/shm/reqwen ]] && qwen "$(cat /dev/shm/reqwen)\n$1" || print "No previous qwen calls found!" ; }'
alias reqwec='() {[[ -f /dev/shm/reqwec ]] && qwec "$(cat /dev/shm/reqwec)\n$1" || print "No previous qwec calls found!" ; }'
alias reqwec='() {[[ -f /dev/shm/reqwec ]] && qwec "$(cat /dev/shm/reqwec)\n$1" || print "No previous qwec calls found!" ; }'
alias remist='() {[[ -f /dev/shm/remist ]] && mist "$(cat /dev/shm/remist)\n$1" || print "No previous mist calls found!" ; }'
alias redeeq='() {[[ -f /dev/shm/redeeq ]] && deeq "$(cat /dev/shm/redeeq)\n$1" || print "No previous deeq calls found!" ; }'
alias regem='() {[[ -f /dev/shm/regem ]] && gem "$(cat /dev/shm/regem)\n$1" || print "No previous gem calls found!" ; }'
alias regem2='() {[[ -f /dev/shm/regem2 ]] && gem2 "$(cat /dev/shm/regem2)\n$1" || print "No previous gem2 calls found!" ; }'

#Here we define the large language models and their parameters. When adding a model, update all arrays correspondingly.
#Avaliable model files (adjust to your situation) that you want to choose from to use with the qlm function and its completion function:
#Declared as global assoc. array to speed up completion ["llmname"]="llmfilename":
typeset -gA llmodels=(
["DeepseekQwen_14B"]="DeepSeek-R1-Distill-Qwen-14B-Q5_K_L.gguf"
["Gemma3_1B"]="gemma-3-1b-it-Q6_K_L.gguf"
["Gemma3_4B"]="gemma-3-4b-it-Q6_K_L.gguf"
["Gemma3_12B"]="gemma-3-12b-it-Q6_K_L.gguf"
["Gemma2_9B"]="gemma-2-9b-it-Q6_K_L.gguf"
["Gemma2_2B"]="gemma-2-2b-it-abliterated-Q6_K_L.gguf"
["Gemma2_27B"]="gemma-2-27b-it-IQ4_XS.gguf"
["MistralSmall3_24B"]="Mistral-Small-24B-Instruct-2501-IQ4_XS.gguf"
["QwenCoder_14B"]="Qwen2.5-Coder-14B-Instruct-Q5_K_L.gguf"
["QwenCoder_32B"]="Qwen2.5-Coder-32B-Instruct-Q5_K_L.gguf"
["Qwen_14B"]="Qwen2.5-14B-Instruct-Q5_K_L.gguf"
["QQwQ_32B"]="Qwen_QwQ-32B-Q5_K_L.gguf"
["Phi4_14B"]="phi-4-Q5_K_L.gguf"
["Phi4_mini"]="Phi-4-mini-instruct-Q6_K_L.gguf"
["Granite3.2_2B"]="granite-3.2-2b-instruct-Q8_0.gguf"
["Llama3.1_8B"]="Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf"
["Llama3.2_3B"]="Llama-3.2-3B-Instruct-Q6_K_L.gguf"
)

#Desired context length (powers of 2), to be adjusted depending on use, model size, VRAM size and gpulayers.            
typeset -A ctxsize=(
["DeepseekQwen_14B"]=4096
["Gemma3_1B"]=32768
["Gemma3_4B"]=16384
["Gemma3_12B"]=6144
["Gemma2_9B"]=8192
["Gemma2_2B"]=8192
["Gemma2_27B"]=4096
["MistralSmall3_24B"]=8192
["QwenCoder_14B"]=4096
["QwenCoder_32B"]=8192
["Qwen_14B"]=4096
["QQwQ_32B"]=8192
["Phi4_14B"]=4096
["Phi4_mini"]=32768
["Granite3.2_2B"]=4096
["Llama3.1_8B"]=8192
["Llama3.2_3B"]=8192
)
#Max context length of each model:
#DeepseekQwen_14B 131072
#Gemma3_1B 32768, 26 blocks
#Gemma3_4B 131072, 48 blocks
#Gemma3_12B 131072, 48 blocks
#Gemma2_9B 8192
#Gemma2_2B 8192
#Gemma2_27B 8192
#MistralSmall3_24B 32768
#QwenCoder_14B 131072
#QwenCoder_32B 131072
#Qwen_14B 131072
#QQwQ_32B 131072
#Phi4_14B 16384
#Phi4_mini 131072
#Granite3.2_2B 131072 
#Llama3.1_8B 131072
#Llama3.2_3B 131072

#Layers to offload to the GPU, to be adjusted depending on model size, VRAM size and desired ctxsize:
typeset -A gpulayers=(
["DeepseekQwen_14B"]=99
["Gemma3_1B"]=99
["Gemma3_4B"]=99
["Gemma3_12B"]=99
["Gemma2_9B"]=99
["Gemma2_2B"]=99
["Gemma2_27B"]=30
["MistralSmall3_24B"]=29
["QwenCoder_14B"]=99
["QwenCoder_32B"]=28
["Qwen_14B"]=99
["QQwQ_32B"]=28
["Phi4_14B"]=99
["Phi4_mini"]=99
["Granite3.2_2B"]=99
["Llama3.1_8B"]=99
["Llama3.2_3B"]=99
)

#Preset temperature parameters for inference, depend on the model and the task:
typeset -A temps=(
["DeepseekQwen_14B"]=0.61
["Gemma3_1B"]=0.16
["Gemma3_4B"]=0.16
["Gemma3_12B"]=0.16
["Gemma2_9B"]=0.16
["Gemma2_2B"]=0.16
["Gemma2_27B"]=0.16
["MistralSmall3_24B"]=0.05
["QwenCoder_14B"]=0.5
["QwenCoder_32B"]=0.5
["Qwen_14B"]=0.61
["QQwQ_32B"]=0.5
["Phi4_14B"]=0.61
["Phi4_mini"]=0.61
["Granite3.2_2B"]=0.61
["Llama3.1_8B"]=0.61
["Llama3.2_3B"]=0.61
)

#Model-dependent prompts with placeholder (UIUIUI) for context(user prompt) ingestion:
typeset -A llmprompts=(
["DeepseekQwen_14B"]="<｜begin▁of▁sentence｜>You are DeepSeekR1-Qwen14B. You are a helpful assistant.<｜User｜>UIUIUI<｜Assistant｜>"
["Gemma3_1B"]="<bos><start_of_turn>user\nYou are Gemma3. You are helpful assistant who answers breafly and to the point.\nUIUIUI<end_of_turn>\n<start_of_turn>model"
["Gemma3_4B"]="<bos><start_of_turn>user\nYou are Gemma3. You are helpful assistant who answers breafly and to the point.\nUIUIUI<end_of_turn>\n<start_of_turn>model"
["Gemma3_12B"]="<bos><start_of_turn>user\nYou are Gemma3. You are helpful assistant who answers breafly and to the point.\nUIUIUI<end_of_turn>\n<start_of_turn>model"
["Gemma2_9B"]="User:\nUIUIUI\nAssistant:"
["Gemma2_2B"]="User:\nUIUIUI\nAssistant:"
["Gemma2_27B"]="User:\nUIUIUI\nAssistant:"
["MistralSmall3_24B"]="[SYSTEM_PROMPT]You are Mistral Small 3, a Large Language Model created by Mistral AI. When you are not sure about some information, you say that you do not have the information.[/SYSTEM_PROMPT][INST]UIUIUI[/INST]"
["QwenCoder_14B"]="<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful coding assistant.<|im_end|>\n<|im_start|>user\nUIUIUI<|im_end|>\n<|im_start|>assistant\n"
["QwenCoder_32B"]="<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful coding assistant.<|im_end|>\n<|im_start|>user\nUIUIUI<|im_end|>\n<|im_start|>assistant\n"
["Qwen_14B"]="<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nUIUIUI<|im_end|>\n<|im_start|>assistant\n"
["QQwQ_32B"]="<|im_start|>system\nYou are Qwen, a helpful coding assistant. Think step by step but only keep a minimum draft of each thinking step, with 5 words at most. Return the answer at the end of the response after a separator ####.<|im_end|>\n<|im_start|>user\nUIUIUI<|im_end|>\n<|im_start|>assistant\n"
["Phi4_14B"]="<|im_start|>system<|im_sep|>You are Phi4, a helpful assistant.<|im_end|><|im_start|>user<|im_sep|>UIUIUI<|im_end|><|im_start|>assistant<|im_sep|>"
["Phi4_mini"]="<|system|>You are Phi4-mini, a helpful assistant.<|end|><|user|>UIUIUI<|end|><|assistant|>"
["Granite3.2_2B"]="<|start_of_role|>system<|end_of_role|>You are Granite, developed by IBM. You are a helpful AI assistant. \nRespond to every user query in a detailed way. You can share your thoughts and reasoning before responding. In the thought process, engage in a comprehensive, iterative cycle of analysis, summarization, exploration, reassessment and reflection. In the response, based on explorations and reflections from the thoughts section, systematically present the final solution that you deem correct. The response should summarize the thought process for each user query.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>UIUIUI<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>\n"
["Llama3.1_8B"]="<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nCutting Knowledge Date: December 2023\nToday Date: $(date +'%d %B %Y')\nYou are Llama3.1, a helpfull assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>UIUIUI<|eot_id|><|start_header_id|>assistant<|end_header_id|>"
["Llama3.2_3B"]="Today is $(date +'%d %b %Y')\nYou are Llama3.2, a helpfull assistant.\nUIUIUI"
)
